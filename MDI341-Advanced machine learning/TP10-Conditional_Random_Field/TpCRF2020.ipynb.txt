{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "TpCRF2020.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnPDS8CN7DJE",
        "colab_type": "text"
      },
      "source": [
        "# CRF\n",
        "In this lab session, you are going to train and test a linear-chain CRF model. Before starting, specify \"python 2\" in the environment parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDQEZO8IkN_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install python-crfsuite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtBfg6D9kJos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import chain\n",
        "import nltk\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import sklearn\n",
        "import pycrfsuite\n",
        "\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbH45JpDkJo0",
        "colab_type": "text"
      },
      "source": [
        "# Let's use CoNLL 2002 data to build a NER system\n",
        "\n",
        "CoNLL2002 corpus is available in NLTK. We use Spanish data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvY2XkMmkJo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('conll2002')\n",
        "nltk.corpus.conll2002.fileids()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG3HyaUykJo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
        "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCajbtaQkJo8",
        "colab_type": "text"
      },
      "source": [
        "Data format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk8sYeOSkJo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sents[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuM9vOXEkJpB",
        "colab_type": "text"
      },
      "source": [
        "## Features\n",
        "\n",
        "Next, define some features. In this example we use word identity, word suffix, word shape and word POS tag; also, some information from nearby words is used. \n",
        "\n",
        "This makes a simple baseline, but you certainly can add and remove some features to get (much?) better results - experiment with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C57u044VkJpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    features = [\n",
        "        'bias',\n",
        "        'word.lower=' + word.lower(),\n",
        "        'word[-3:]=' + word[-3:],\n",
        "        'word[-2:]=' + word[-2:],\n",
        "        'word.isupper=%s' % word.isupper(),\n",
        "        'word.istitle=%s' % word.istitle(),\n",
        "        'word.isdigit=%s' % word.isdigit(),\n",
        "        'postag=' + postag,\n",
        "        'postag[:2]=' + postag[:2],\n",
        "    ]\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.extend([\n",
        "            '-1:word.lower=' + word1.lower(),\n",
        "            '-1:word.istitle=%s' % word1.istitle(),\n",
        "            '-1:word.isupper=%s' % word1.isupper(),\n",
        "            '-1:postag=' + postag1,\n",
        "            '-1:postag[:2]=' + postag1[:2],\n",
        "        ])\n",
        "    else:\n",
        "        features.append('BOS')\n",
        "        \n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.extend([\n",
        "            '+1:word.lower=' + word1.lower(),\n",
        "            '+1:word.istitle=%s' % word1.istitle(),\n",
        "            '+1:word.isupper=%s' % word1.isupper(),\n",
        "            '+1:postag=' + postag1,\n",
        "            '+1:postag[:2]=' + postag1[:2],\n",
        "        ])\n",
        "    else:\n",
        "        features.append('EOS')\n",
        "                \n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLs1qdrwkJpF",
        "colab_type": "text"
      },
      "source": [
        "This is what word2features extracts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91I8-hWKkJpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent2features(train_sents[0])[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFFfLZfhkJpK",
        "colab_type": "text"
      },
      "source": [
        "Extract the features from the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qX9xHIZkJpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOv437XRkJpO",
        "colab_type": "text"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "To train the model, we create pycrfsuite.Trainer, load the training data and call 'train' method. \n",
        "First, create pycrfsuite.Trainer and load the training data to CRFsuite:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwDq8X7JkJpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "trainer = pycrfsuite.Trainer(verbose=False)\n",
        "\n",
        "for xseq, yseq in zip(X_train, y_train):\n",
        "    trainer.append(xseq, yseq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdmrrOL5kJpT",
        "colab_type": "text"
      },
      "source": [
        "Set training parameters. We will use L-BFGS training algorithm (it is default) with Elastic Net (L1 + L2) regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7DFpR-nkJpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.set_params({\n",
        "    'c1': 1.0,   # coefficient for L1 penalty\n",
        "    'c2': 1e-3,  # coefficient for L2 penalty\n",
        "    'max_iterations': 50,  # stop earlier\n",
        "\n",
        "    # include transitions that are possible, but not observed\n",
        "    'feature.possible_transitions': True\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha1kCzPOkJpY",
        "colab_type": "text"
      },
      "source": [
        "Possible parameters for the default training algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxAhnb4JkJpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLfHHcNhkJpb",
        "colab_type": "text"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpRk1MYNkJpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "trainer.train('conll2002-esp.crfsuite')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn-frGmPkJpf",
        "colab_type": "text"
      },
      "source": [
        "trainer.train saves model to a file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFZMoofQkJpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lh ./conll2002-esp.crfsuite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkOsyFJBkJpk",
        "colab_type": "text"
      },
      "source": [
        "We can also get information about the final state of the model by looking at the trainer's logparser. If we had tagged our input data using the optional group argument in add, and had used the optional holdout argument during train, there would be information about the trainer's performance on the holdout set as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmD4GyyXkJpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.logparser.last_iteration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFrun6RckJpo",
        "colab_type": "text"
      },
      "source": [
        "We can also get this information for every step using trainer.logparser.iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65H7IPAokJpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print len(trainer.logparser.iterations), trainer.logparser.iterations[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OjCvmWokJpt",
        "colab_type": "text"
      },
      "source": [
        "## Make predictions\n",
        "\n",
        "To use the trained model, create pycrfsuite.Tagger, open the model and use \"tag\" method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFbI7XmtkJpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagger = pycrfsuite.Tagger()\n",
        "tagger.open('conll2002-esp.crfsuite')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbXVsuT_kJpy",
        "colab_type": "text"
      },
      "source": [
        "Let's tag a sentence to see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEj3py5OkJpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_sent = test_sents[0]\n",
        "print(' '.join(sent2tokens(example_sent)))\n",
        "\n",
        "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
        "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iRBHaTnkJp2",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaRU95LrkJp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bio_classification_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Classification report for a list of BIO-encoded sequences.\n",
        "    It computes token-level metrics and discards \"O\" labels.\n",
        "    \n",
        "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
        "    to calculate averages properly!\n",
        "    \"\"\"\n",
        "    lb = LabelBinarizer()\n",
        "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
        "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
        "        \n",
        "    tagset = set(lb.classes_) - {'O'}\n",
        "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
        "    \n",
        "    return classification_report(\n",
        "        y_true_combined,\n",
        "        y_pred_combined,\n",
        "        labels = [class_indices[cls] for cls in tagset],\n",
        "        target_names = tagset,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8tGRwJgkJp5",
        "colab_type": "text"
      },
      "source": [
        "Predict entity labels for all sentences in our testing set ('testb' Spanish data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8WG0fFPkJp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "y_pred = [tagger.tag(xseq) for xseq in X_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7yHpsvYkJp9",
        "colab_type": "text"
      },
      "source": [
        "..and check the result. Note this report is not comparable to results in CONLL2002 papers because here we check per-token results (not per-entity). Per-entity numbers will be worse.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XoxKtpDkJp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(bio_classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNkRLt9QkJqA",
        "colab_type": "text"
      },
      "source": [
        "## Let's check what classifier learned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP_8yd-tkJqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "info = tagger.info()\n",
        "\n",
        "def print_transitions(trans_features):\n",
        "    for (label_from, label_to), weight in trans_features:\n",
        "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
        "\n",
        "print(\"Top likely transitions:\")\n",
        "print_transitions(Counter(info.transitions).most_common(15))\n",
        "\n",
        "print(\"\\nTop unlikely transitions:\")\n",
        "print_transitions(Counter(info.transitions).most_common()[-15:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I29MFUWTkJqH",
        "colab_type": "text"
      },
      "source": [
        "We can see that, for example, it is very likely that the beginning of an organization name (B-ORG) will be followed by a token inside organization name (I-ORG), but transitions to I-ORG from tokens with other labels are penalized. Also note I-PER -> B-LOC transition: a positive weight means that model thinks that a person name is often followed by a location.\n",
        "\n",
        "Check the state features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZw9bui6kJqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_state_features(state_features):\n",
        "    for (attr, label), weight in state_features:\n",
        "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
        "\n",
        "print(\"Top positive:\")\n",
        "print_state_features(Counter(info.state_features).most_common(20))\n",
        "\n",
        "print(\"\\nTop negative:\")\n",
        "print_state_features(Counter(info.state_features).most_common()[-20:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJBLgtFjkJqM",
        "colab_type": "text"
      },
      "source": [
        "Some observations:\n",
        "\n",
        "* **8.743642 B-ORG  word.lower=psoe-progresistas** - the model remembered names of some entities - maybe it is overfit, or maybe our features are not adequate, or maybe remembering is indeed helpful;\n",
        "* **5.195429 I-LOC  -1:word.lower=calle**: \"calle\" is a street in Spanish; model learns that if a previous word was \"calle\" then the token is likely a part of location;\n",
        "* **-3.529449 O      word.isupper=True**, ** -2.913103 O      word.istitle=True **: UPPERCASED or TitleCased words are likely entities of some kind;\n",
        "* **-2.585756 O      postag=NP** - proper nouns (NP is a proper noun in the Spanish tagset) are often entities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DLlEfqFkJqM",
        "colab_type": "text"
      },
      "source": [
        "# Coding your own CRF inference routine\n",
        "\n",
        "To help you, we use a library, named flexcrf, that provide some inference routines you will use to test your viterbi algorithm. With the following command, you donwload and unzip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-dnGzRqhR0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!if [[ ! -d flexcrf_tp ]]; then wget http://stelat.eu/wp-content/uploads/2020/03/flexcrf_tp.zip && unzip flexcrf_tp.zip;fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcsfe16WAnqQ",
        "colab_type": "text"
      },
      "source": [
        "Here, we import some functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzR32ulEAmbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cPickle as pickle\n",
        "\n",
        "import numpy as np\n",
        "from pycrfsuite import Tagger\n",
        "from flexcrf_tp.models.linear_chain import (_feat_fun_values,\n",
        "                                            _compute_all_potentials,\n",
        "                                            _forward_score,\n",
        "                                            _backward_score,\n",
        "                                            _partition_fun_value,\n",
        "                                            _posterior_score)\n",
        "\n",
        "from flexcrf_tp.crfsuite2flexcrf import convert_data_to_flexcrf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWsbJAlj_7hj",
        "colab_type": "text"
      },
      "source": [
        "#Viterbi decoder\n",
        "Now you can complete the viterbi_decoder function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1nejpKuAD6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# -- Define vitrebi_decoder here:\n",
        "\n",
        "def viterbi_decoder(m_xy, n=None, log_version=True):\n",
        "    \"\"\"\n",
        "    Performs MAP inference, determining $y = \\argmax_y P(y|x)$, using the\n",
        "    Viterbi algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m_xy : ndarray, shape (n_obs, n_labels, n_labels)\n",
        "        Values of log-potentials ($\\log M_i(y_{i-1}, y_i, x)$)\n",
        "        computed based on feature functions f_xy and/or user-defined potentials\n",
        "        `psi_xy`. At t=0, m_xy[0, 0, :] contains values of $\\log M_1(y_0, y_1)$\n",
        "        with $y_0$ the fixed initial state.\n",
        "\n",
        "    n : integer, default=None\n",
        "        Time position up to which to decode the optimal sequence; if not\n",
        "        specified (default) the score is computed for the whole sequence.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_pred : ndarray, shape (n_obs,)\n",
        "        Predicted optimal sequence of labels.\n",
        "\n",
        "    TODO: Cythonise this function for more efficiency.\n",
        "    \"\"\"\n",
        "\n",
        "    if n is None:\n",
        "        n = m_xy.shape[0]\n",
        "\n",
        "    # Here we provide the temporary variables required by the viterbi algorithm.\n",
        "    n_labels = m_xy.shape[2]\n",
        "    y_pred = np.empty(n, dtype=int)\n",
        "    delta = np.empty((n, n_labels))\n",
        "    delta[0, :] = m_xy[0, 0, :]\n",
        "    btrack = np.empty((n, n_labels), dtype=int)\n",
        "\n",
        "    # Viterbi scores\n",
        "    #YOUR CODE HERE\n",
        "    \n",
        "    # Backtracking\n",
        "    #YOUR CODE HERE\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2VpfF_YASHp",
        "colab_type": "text"
      },
      "source": [
        "# Test your Viterbi decoder\n",
        "Check if you viterbi decoder provide the same output as pycrfsuite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ3MjyIBgxKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# -- Load data and crfsuite model and convert them-------------------------\n",
        "\n",
        "RECREATE = True  # set to True to recreate flexcrf data with new model\n",
        "\n",
        "CRFSUITE_MODEL_FILE = './conll2002-esp.crfsuite'\n",
        "CRFSUITE_MODEL_INFO_FILE = './conll2002-esp.crfsuite-model-info.dump'\n",
        "\n",
        "CRFSUITE_TEST_DATA_FILE = './conll2002-esp_crfsuite-test-data.dump'\n",
        "FLEXCRF_TEST_DATA_FILE = './conll2002-esp_flexcrf-test-data.dump'\n",
        "\n",
        "# crfsuite model\n",
        "tagger = Tagger()\n",
        "tagger.open(CRFSUITE_MODEL_FILE)\n",
        "model = tagger.info()\n",
        "#model = pickle.load(open(CRFSUITE_MODEL_INFO_FILE))\n",
        "#print \"model loaded.\"\n",
        "data={'X': X_test, 'y': y_test}\n",
        "#data = pickle.load(open(CRFSUITE_TEST_DATA_FILE))\n",
        "#print \"test data loaded.\"\n",
        "\n",
        "if RECREATE:\n",
        "    dataset, thetas = convert_data_to_flexcrf(data, model, n_seq=3)\n",
        "    pickle.dump({'dataset': dataset, 'thetas': thetas},\n",
        "                open(FLEXCRF_TEST_DATA_FILE, 'wb'))\n",
        "else:\n",
        "    dd = pickle.load(open(FLEXCRF_TEST_DATA_FILE))\n",
        "    dataset = dd['dataset']\n",
        "    thetas = dd['thetas']\n",
        "\n",
        "# -- Start classification ------------------------------------------------\n",
        "\n",
        "for seq in range(len(dataset)):\n",
        "\n",
        "    # -- with pycrfsuite\n",
        "    s_ = tagger.tag(data['X'][seq])\n",
        "    y_ = np.array([int(model.labels[s]) for s in s_])\n",
        "    prob_ = tagger.probability(s_)\n",
        "\n",
        "    print \"\\n-- With crfsuite:\"\n",
        "    print \"labels:\\n\", s_, \"\\n\", y_\n",
        "    print \"probability:\\t %f\" % prob_\n",
        "\n",
        "    # -- with flexcrf\n",
        "    f_xy, y = dataset[seq]\n",
        "    \n",
        "    theta = thetas[seq]\n",
        "\n",
        "    m_xy, f_m_xy = _compute_all_potentials(f_xy, theta)\n",
        "\n",
        "    y_pred = viterbi_decoder(m_xy)\n",
        "\n",
        "    alpha = _forward_score(m_xy)\n",
        "    #beta = _backward_score(m_xy)\n",
        "    z_x = _partition_fun_value(alpha)\n",
        "\n",
        "    # compare flexcrf prob to crfsuill2002-esp.crfsuite-te prob\n",
        "    f_x = _feat_fun_values(f_xy, y_, with_f_x_sum=False)\n",
        "    prob0 = np.exp(_posterior_score(f_x=f_x, theta=theta, z_x=z_x))\n",
        "    print \"flexcrf prob:\\t %f\" % prob0\n",
        "\n",
        "    f_x = _feat_fun_values(f_xy, y_pred, with_f_x_sum=False)\n",
        "    prob = np.exp(_posterior_score(f_x=f_x, theta=theta, z_x=z_x))\n",
        "\n",
        "    print \"-- With flexcrf:\"\n",
        "    print \"labels:\\n\", y_pred\n",
        "    print \"equal predictions: \", all(y_pred == y_)\n",
        "    print \"probability:\\t %f\" % prob\n",
        "    print \"delta:\\t %f\" % abs(prob-prob_)\n",
        "\n",
        "tagger.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}