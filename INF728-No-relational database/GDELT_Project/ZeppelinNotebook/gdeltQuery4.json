{"paragraphs":[{"text":"%md\n## Querry 4\nHere we will try to resolve the following query:\n\nDraw the map of relation between countries based on the average tone of article writen in one langage about an event in an other country. Compute the number of article and the average tone. Enable filter on year, month or day and country. \n\nWe will use the table event and mention to answer this query.","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Querry 4</h2>\n<p>Here we will try to resolve the following query:</p>\n<p>Draw the map of relation between countries based on the average tone of article writen in one langage about an event in an other country. Compute the number of article and the average tone. Enable filter on year, month or day and country. </p>\n<p>We will use the table event and mention to answer this query.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579706892656_2048828653","id":"20200120-084017_440117845","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1619"},{"text":"%md Edit the interpreter spark (top right drop down menu) and add those two variables:\n```\nspark.jars.packages                         datastax:spark-cassandra-connector:2.4.0-s_2.11\nspark.cassandra.connection.host             private-ip-cassandra-node-1,private-ip-cassandra-node-2 \n```","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Edit the interpreter spark (top right drop down menu) and add those two variables:</p>\n<pre><code>spark.jars.packages                         datastax:spark-cassandra-connector:2.4.0-s_2.11\nspark.cassandra.connection.host             private-ip-cassandra-node-1,private-ip-cassandra-node-2 \n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1579706892657_-1571185092","id":"20200120-084650_1382961411","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1620"},{"text":"import org.apache.spark.{SparkConf, SparkContext, sql}\nimport org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport org.apache.spark.rdd.RDD\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicSessionCredentials\nimport java.io.File\n//Cassandra\n// import com.datastax.spark.connector.cql.CassandraConnector\n// import org.apache.spark.sql.cassandra._\n","user":"anonymous","dateUpdated":"2020-01-22T15:28:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.{SparkConf, SparkContext, sql}\nimport org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport org.apache.spark.rdd.RDD\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicSessionCredentials\nimport java.io.File\n"}]},"apps":[],"jobName":"paragraph_1579706892657_-305024805","id":"20200120-090150_119866461","dateCreated":"2020-01-22T15:28:12+0000","dateStarted":"2020-01-22T15:28:31+0000","dateFinished":"2020-01-22T15:28:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1621"},{"text":"val AWS_ID = \"ASIAX7FDIQZHUCRPA4ES\"\nval AWS_KEY = \"Uz6aHDfNnDI1NOcv8U/BEcHbwU5rAStsJFv7ZsMz\"\nval AWS_TOKEN = \"FwoGZXIvYXdzEKn//////////wEaDBbHR/f82PWkv7dYkCK/ASnJ8mvAGxkQjNbJZnmhQPURWPEURjIC2JTZ4XUe8OuTgSaCtnPXYBUWG6NvmP3D5vhz74GnVXqaYnK8jbsKPd0aCqyYl8GPKP0+u2XZ42PDEmHyC02RgRaY95fEZRFKCycqRMH2oGqZRIDLxRt+0UWyLsL2mA7MNawzoH6DwCbHc1BH5g6Vg5mrOILLK/jF6OnMBZvQQX7XKlCq4mVkWUhn6pvP7mVOhCD0Zo/NeR9t1nNdLgGB32kPOXlbxh6vKLjUofEFMi0Q5x4JogkVbdcsnvN+XmrPGhhCQ2EHn2S0cKQpCy8Woh/xjHtTPam7JASjM/E=\"\nval s3_name = \"projet-gdelt-2019\"\n\nsc.hadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.session.token\", AWS_TOKEN)","user":"anonymous","dateUpdated":"2020-01-22T15:29:38+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"AWS_ID: String = ASIAX7FDIQZHUCRPA4ES\nAWS_KEY: String = Uz6aHDfNnDI1NOcv8U/BEcHbwU5rAStsJFv7ZsMz\nAWS_TOKEN: String = FwoGZXIvYXdzEKn//////////wEaDBbHR/f82PWkv7dYkCK/ASnJ8mvAGxkQjNbJZnmhQPURWPEURjIC2JTZ4XUe8OuTgSaCtnPXYBUWG6NvmP3D5vhz74GnVXqaYnK8jbsKPd0aCqyYl8GPKP0+u2XZ42PDEmHyC02RgRaY95fEZRFKCycqRMH2oGqZRIDLxRt+0UWyLsL2mA7MNawzoH6DwCbHc1BH5g6Vg5mrOILLK/jF6OnMBZvQQX7XKlCq4mVkWUhn6pvP7mVOhCD0Zo/NeR9t1nNdLgGB32kPOXlbxh6vKLjUofEFMi0Q5x4JogkVbdcsnvN+XmrPGhhCQ2EHn2S0cKQpCy8Woh/xjHtTPam7JASjM/E=\ns3_name: String = projet-gdelt-2019\n"}]},"apps":[],"jobName":"paragraph_1579706892657_1852558581","id":"20200120-085958_2047207204","dateCreated":"2020-01-22T15:28:12+0000","dateStarted":"2020-01-22T15:29:38+0000","dateFinished":"2020-01-22T15:29:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1622"},{"text":"%md ### Exportation of the tables","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Exportation of the tables</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1579706892658_2087234230","id":"20200120-084655_319078162","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1623"},{"text":"/** **************************************\n * Charger un fichier csv dans un rdd depuis s3\n * ********************************************/\n\n// *** Events ***\nval textRDDEvents: RDD[String] = sc.binaryFiles(\"s3://\" + s3_name + \"/2019*.export.CSV.zip,s3://\" + s3_name + \"/2019*.translation.export.CSV.zip\").\n  flatMap { // decompresser les fichiers\n    case (name: String, content: PortableDataStream) =>\n      val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n          takeWhile{ case null => zis.close(); false\n                       case _ => true }.\n          flatMap { _ =>\n            val br = new BufferedReader(new InputStreamReader(zis))\n            Stream.continually(br.readLine()).takeWhile(_ != null)\n          } \n    }\n\n// val textRDDEvents: RDD[String] = sc.binaryFiles(\"s3://\" + s3_name + \"/201912*.export.CSV.zip,s3://\" + s3_name + \"/201912*.translation.export.CSV.zip\").\n//   flatMap { // decompresser les fichiers\n//     t =>\n//       try{\n//           val zis = new ZipInputStream(t._2.open)\n//           Stream.continually(zis.getNextEntry).\n//           takeWhile(_ != null).\n//           flatMap { _ =>\n//             val br = new BufferedReader(new InputStreamReader(zis))\n//             Stream.continually(br.readLine()).takeWhile(_ != null)\n//           } \n          \n//         } finally{\n//               t._2.close()\n//         }\n//     }\n\n\n// *** Mentions ***\nval textRDDMentions: RDD[String] = sc.binaryFiles(\"s3://\" + s3_name + \"/2019*.mentions.CSV.zip,s3://\" + s3_name + \"/2019*.translation.mentions.CSV.zip\").\n  flatMap { // decompresser les fichiers\n    case (name: String, content: PortableDataStream) =>\n      val zis = new ZipInputStream(content.open)\n      Stream.continually(zis.getNextEntry).\n        takeWhile{ case null => zis.close(); false\n                   case _ => true }.\n        flatMap { _ =>\n          val br = new BufferedReader(new InputStreamReader(zis))\n          Stream.continually(br.readLine()).takeWhile(_ != null)\n        }\n  }\n  \n  \n  /** ************************************************************\n * Ajout des informations colonnes et creation d'un Dataframe\n * ************************************************************* */\n\n// *** Events ***\nval dfEvents: DataFrame = textRDDEvents.toDF.withColumn(\"GLOBALEVENTID\", split($\"value\", \"\\\\t\").getItem(0))\n  .withColumn(\"Day\", split($\"value\", \"\\\\t\").getItem(1))\n  .withColumn(\"MonthYear\", split($\"value\", \"\\\\t\").getItem(2))\n  .withColumn(\"Year\", split($\"value\", \"\\\\t\").getItem(3))\n  .withColumn(\"FractionDate\", split($\"value\", \"\\\\t\").getItem(4))\n  .withColumn(\"Actor1Code\", split($\"value\", \"\\\\t\").getItem(5))\n  .withColumn(\"Actor1Name\", split($\"value\", \"\\\\t\").getItem(6))\n  .withColumn(\"Actor1CountryCode\", split($\"value\", \"\\\\t\").getItem(7))\n  .withColumn(\"Actor1KnownGroupCode\", split($\"value\", \"\\\\t\").getItem(8))\n  .withColumn(\"Actor1EthnicCode\", split($\"value\", \"\\\\t\").getItem(9))\n  .withColumn(\"Actor1Religion1Code\", split($\"value\", \"\\\\t\").getItem(10))\n  .withColumn(\"Actor1Religion2Code\", split($\"value\", \"\\\\t\").getItem(11))\n  .withColumn(\"Actor1Type1Code\", split($\"value\", \"\\\\t\").getItem(12))\n  .withColumn(\"Actor1Type2Code\", split($\"value\", \"\\\\t\").getItem(13))\n  .withColumn(\"Actor1Type3Code\", split($\"value\", \"\\\\t\").getItem(14))\n  .withColumn(\"Actor2Code\", split($\"value\", \"\\\\t\").getItem(15))\n  .withColumn(\"Actor2Name\", split($\"value\", \"\\\\t\").getItem(16))\n  .withColumn(\"Actor2CountryCode\", split($\"value\", \"\\\\t\").getItem(17))\n  .withColumn(\"Actor2KnownGroupCode\", split($\"value\", \"\\\\t\").getItem(18))\n  .withColumn(\"Actor2EthnicCode\", split($\"value\", \"\\\\t\").getItem(19))\n  .withColumn(\"Actor2Religion1Code\", split($\"value\", \"\\\\t\").getItem(20))\n  .withColumn(\"Actor2Religion2Code\", split($\"value\", \"\\\\t\").getItem(21))\n  .withColumn(\"Actor2Type1Code\", split($\"value\", \"\\\\t\").getItem(22))\n  .withColumn(\"Actor2Type2Code\", split($\"value\", \"\\\\t\").getItem(23))\n  .withColumn(\"Actor2Type3Code\", split($\"value\", \"\\\\t\").getItem(24))\n  .withColumn(\"IsRootEvent\", split($\"value\", \"\\\\t\").getItem(25))\n  .withColumn(\"EventCode\", split($\"value\", \"\\\\t\").getItem(26))\n  .withColumn(\"EventBaseCode\", split($\"value\", \"\\\\t\").getItem(27))\n  .withColumn(\"EventRootCode\", split($\"value\", \"\\\\t\").getItem(28))\n  .withColumn(\"QuadClass\", split($\"value\", \"\\\\t\").getItem(29))\n  .withColumn(\"GoldsteinScale\", split($\"value\", \"\\\\t\").getItem(30))\n  .withColumn(\"NumMentions\", split($\"value\", \"\\\\t\").getItem(31))\n  .withColumn(\"NumSources\", split($\"value\", \"\\\\t\").getItem(32))\n  .withColumn(\"NumArticles\", split($\"value\", \"\\\\t\").getItem(33))\n  .withColumn(\"AvgTone\", split($\"value\", \"\\\\t\").getItem(34))\n  .withColumn(\"Actor1Geo_Type\", split($\"value\", \"\\\\t\").getItem(35))\n  .withColumn(\"Actor1Geo_FullName\", split($\"value\", \"\\\\t\").getItem(36))\n  .withColumn(\"Actor1Geo_CountryCode\", split($\"value\", \"\\\\t\").getItem(37))\n  .withColumn(\"Actor1Geo_ADM1Code\", split($\"value\", \"\\\\t\").getItem(38))\n  .withColumn(\"Actor1Geo_ADM2Code\", split($\"value\", \"\\\\t\").getItem(39))\n  .withColumn(\"Actor1Geo_Lat\", split($\"value\", \"\\\\t\").getItem(40))\n  .withColumn(\"Actor1Geo_Long\", split($\"value\", \"\\\\t\").getItem(41))\n  .withColumn(\"Actor1Geo_FeatureID\", split($\"value\", \"\\\\t\").getItem(42))\n  .withColumn(\"Actor2Geo_Type\", split($\"value\", \"\\\\t\").getItem(43))\n  .withColumn(\"Actor2Geo_FullName\", split($\"value\", \"\\\\t\").getItem(44))\n  .withColumn(\"Actor2Geo_CountryCode\", split($\"value\", \"\\\\t\").getItem(45))\n  .withColumn(\"Actor2Geo_ADM1Code\", split($\"value\", \"\\\\t\").getItem(46))\n  .withColumn(\"Actor2Geo_ADM2Code\", split($\"value\", \"\\\\t\").getItem(47))\n  .withColumn(\"Actor2Geo_Lat\", split($\"value\", \"\\\\t\").getItem(48))\n  .withColumn(\"Actor2Geo_Long\", split($\"value\", \"\\\\t\").getItem(49))\n  .withColumn(\"Actor2Geo_FeatureID\", split($\"value\", \"\\\\t\").getItem(50))\n  .withColumn(\"ActionGeo_Type\", split($\"value\", \"\\\\t\").getItem(51))\n  .withColumn(\"ActionGeo_FullName\", split($\"value\", \"\\\\t\").getItem(52))\n  .withColumn(\"ActionGeo_CountryCode\", split($\"value\", \"\\\\t\").getItem(53))\n  .withColumn(\"ActionGeo_ADM1Code\", split($\"value\", \"\\\\t\").getItem(54))\n  .withColumn(\"ActionGeo_ADM2Code\", split($\"value\", \"\\\\t\").getItem(55))\n  .withColumn(\"ActionGeo_Lat\", split($\"value\", \"\\\\t\").getItem(56))\n  .withColumn(\"ActionGeo_Long\", split($\"value\", \"\\\\t\").getItem(57))\n  .withColumn(\"ActionGeo_FeatureID\", split($\"value\", \"\\\\t\").getItem(58))\n  .withColumn(\"DATEADDED\", split($\"value\", \"\\\\t\").getItem(59))\n  .withColumn(\"SOURCEURL\", split($\"value\", \"\\\\t\").getItem(60))\n  .drop(\"value\")\n\n// *** Mentions ***\nval dfMentions: DataFrame = textRDDMentions.toDF.withColumn(\"GLOBALEVENTID\", split($\"value\", \"\\\\t\").getItem(0))\n  .withColumn(\"GLOBALEVENTID\", split($\"value\", \"\\\\t\").getItem(0))\n  .withColumn(\"EventTimeDate\", split($\"value\", \"\\\\t\").getItem(1))\n  .withColumn(\"MentionTimeDate\", split($\"value\", \"\\\\t\").getItem(2))\n  .withColumn(\"MentionType\", split($\"value\", \"\\\\t\").getItem(3))\n  .withColumn(\"MentionSourceName\", split($\"value\", \"\\\\t\").getItem(4))\n  .withColumn(\"MentionIdentifier\", split($\"value\", \"\\\\t\").getItem(5))\n  .withColumn(\"SentenceID\", split($\"value\", \"\\\\t\").getItem(6))\n  .withColumn(\"Actor1CharOffset\", split($\"value\", \"\\\\t\").getItem(7))\n  .withColumn(\"Actor2CharOffset\", split($\"value\", \"\\\\t\").getItem(8))\n  .withColumn(\"ActionCharOffset\", split($\"value\", \"\\\\t\").getItem(9))\n  .withColumn(\"InRawText\", split($\"value\", \"\\\\t\").getItem(10))\n  .withColumn(\"Confidence\", split($\"value\", \"\\\\t\").getItem(11))\n  .withColumn(\"MentionDocLen\", split($\"value\", \"\\\\t\").getItem(12))\n  .withColumn(\"MentionDocTone\", split($\"value\", \"\\\\t\").getItem(13))\n  .withColumn(\"MentionDocTranslationInfo\", split($\"value\", \"\\\\t\").getItem(14))\n  .withColumn(\"Extras\", split($\"value\", \"\\\\t\").getItem(15))\n  .drop(\"value\")\n","user":"anonymous","dateUpdated":"2020-01-22T15:48:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"textRDDEvents: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[177] at flatMap at <console>:75\ntextRDDMentions: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[179] at flatMap at <console>:107\ndfEvents: org.apache.spark.sql.DataFrame = [GLOBALEVENTID: string, Day: string ... 59 more fields]\ndfMentions: org.apache.spark.sql.DataFrame = [GLOBALEVENTID: string, EventTimeDate: string ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579706892658_1345069094","id":"20200120-085423_780954602","dateCreated":"2020-01-22T15:28:12+0000","dateStarted":"2020-01-22T15:48:16+0000","dateFinished":"2020-01-22T15:48:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1624"},{"text":"%md Processing of the data","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Processing of the data</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579706892658_-184234815","id":"20200120-101202_1214447844","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1625"},{"text":"val df_country_map = dfEvents\n    .select(\"GLOBALEVENTID\", \"Day\", \"ActionGeo_CountryCode\") //\"ActionGeo_Lat\", \"ActionGeo_Long\"\n    .filter(!($\"ActionGeo_CountryCode\".isNaN || $\"ActionGeo_CountryCode\".isNull || $\"ActionGeo_CountryCode\" === \"\"))\n    .join(\n        dfMentions\n        .select(\"GLOBALEVENTID\", \"MentionDocTone\", \"MentionDocTranslationInfo\"), \"GLOBALEVENTID\")\n        .filter(!($\"MentionDocTranslationInfo\".isNaN || $\"MentionDocTranslationInfo\".isNull || $\"MentionDocTranslationInfo\" === \"\"))\n    .withColumn(\"translation_info\", substring($\"MentionDocTranslationInfo\", 7, 3))\n    .groupBy(\"Day\", \"ActionGeo_CountryCode\", \"translation_info\") //\"ActionGeo_Lat\", \"ActionGeo_Long\"\n    .agg(\n        count(\"GLOBALEVENTID\").alias(\"num_article\"), \n        sum(\"MentionDocTone\").alias(\"average_tone\"))\n    .withColumn(\"average_tone\", round($\"average_tone\".cast(\"float\")))\n    .withColumn(\"year\", substring($\"Day\", 0, 4))\n    .withColumn(\"month\", substring($\"Day\", 5, 2))\n    .withColumn(\"day\", substring($\"Day\", 7, 2))\n    .withColumnRenamed(\"ActionGeo_CountryCode\", \"country_code\")\n    ","user":"anonymous","dateUpdated":"2020-01-22T15:30:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df_country_map: org.apache.spark.sql.DataFrame = [day: string, country_code: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579706892658_1764922834","id":"20200120-103420_1969310222","dateCreated":"2020-01-22T15:28:12+0000","dateStarted":"2020-01-22T15:30:24+0000","dateFinished":"2020-01-22T15:30:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1626"},{"text":"df_country_map.show(5)","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+------------+----------------+-----------+------------+----+-----+\n|day|country_code|translation_info|num_article|average_tone|year|month|\n+---+------------+----------------+-----------+------------+----+-----+\n| 01|          RS|             ces|          1|        -3.0|2019|   12|\n| 01|          CH|             nep|          2|         1.0|2019|   12|\n| 01|          IC|             swe|          2|         1.0|2019|   12|\n| 01|          US|             ces|          1|         0.0|2019|   12|\n| 24|          FR|             tur|          2|        -6.0|2019|   11|\n+---+------------+----------------+-----------+------------+----+-----+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1579706892659_-2066595260","id":"20200120-093847_841780827","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1627"},{"text":"%md Long term save in S3 with parquet","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Long term save in S3 with parquet</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579706892659_-1591853729","id":"20200120-101243_23928281","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1628"},{"text":"df_country_map\n  .write\n  .mode(SaveMode.Overwrite)\n  .parquet(\"s3://\" + s3_name + \"/country_map.parquet/\")","user":"anonymous","dateUpdated":"2020-01-22T15:30:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n  ... 56 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 24.0 failed 4 times, most recent failure: Lost task 58.3 in stage 24.0 (TID 832, ip-172-31-34-188.ec2.internal, executor 4): java.io.IOException: Stream closed\n\tat java.util.zip.ZipInputStream.ensureOpen(ZipInputStream.java:67)\n\tat java.util.zip.ZipInputStream.closeEntry(ZipInputStream.java:139)\n\tat $anonfun$1.apply(<console>:85)\n\tat $anonfun$1.apply(<console>:72)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n  ... 78 more\nCaused by: java.io.IOException: Stream closed\n  at java.util.zip.ZipInputStream.ensureOpen(ZipInputStream.java:67)\n  at java.util.zip.ZipInputStream.closeEntry(ZipInputStream.java:139)\n  at $anonfun$1.apply(<console>:85)\n  at $anonfun$1.apply(<console>:72)\n  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"}]},"apps":[],"jobName":"paragraph_1579706892659_1897591358","id":"20200120-101305_2024586033","dateCreated":"2020-01-22T15:28:12+0000","dateStarted":"2020-01-22T15:30:37+0000","dateFinished":"2020-01-22T15:39:29+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:1629"},{"text":"%md Creation of Cassandra table ","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Creation of Cassandra table</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579706892660_166331654","id":"20200120-094026_819957452","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1630"},{"text":"CassandraConnector(sc.getConf).withSessionDo { session =>\n  session.execute(\n    \"\"\"\n       CREATE KEYSPACE IF NOT EXISTS gdelt\n       WITH REPLICATION =\n       {'class': 'SimpleStrategy', 'replication_factor': 2 };\n    \"\"\")\n  session.execute(\n    \"\"\"\n        CREATE TABLE IF NOT EXISTS gdelt.country_map (\n            country_code text,\n            translation_info text,\n            year int,\n            month int,\n            date int,\n            num_article int,\n            sum_tone float,\n            PRIMARY KEY ((translation_info, country_code), year, month, day)\n        );\n    \"\"\"\n  )\n}","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579706892660_1180287112","id":"20200120-101617_1390205","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1631"},{"text":"%md Import of the dataframe data in Cassandra","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Import of the dataframe data in Cassandra</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579706892660_1627548654","id":"20200120-104314_1618938528","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1632"},{"text":"df_country_map.write\n  .cassandraFormat(\"country_map\", \"gdelt\")\n  .save()","user":"anonymous","dateUpdated":"2020-01-22T15:28:12+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579706892660_-1538270078","id":"20200120-104229_259770283","dateCreated":"2020-01-22T15:28:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1633"}],"name":"gdeltQuery4","id":"2F1R853DY","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}